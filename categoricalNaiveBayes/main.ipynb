{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-29T05:17:10.112164Z",
     "iopub.status.busy": "2025-10-29T05:17:10.111336Z",
     "iopub.status.idle": "2025-10-29T05:17:13.146804Z",
     "shell.execute_reply": "2025-10-29T05:17:13.145928Z",
     "shell.execute_reply.started": "2025-10-29T05:17:10.112135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix,accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,chi2\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "df = pd.read_csv('./train1.csv')\n",
    "X_train = df.drop(['target','id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=29173, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88863, 65)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_test = pd.read_csv('test.csv')\\nX_test = df_test.drop('id',axis=1) \\ntest_ids = df_test['id']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df_test = pd.read_csv('test.csv')\n",
    "X_test = df_test.drop('id',axis=1) \n",
    "test_ids = df_test['id']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T05:17:20.743159Z",
     "iopub.status.busy": "2025-10-29T05:17:20.742847Z",
     "iopub.status.idle": "2025-10-29T05:17:20.858003Z",
     "shell.execute_reply": "2025-10-29T05:17:20.856980Z",
     "shell.execute_reply.started": "2025-10-29T05:17:20.743135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in X_train.columns if col.endswith('_cat')]\n",
    "X_train[categorical_cols] = X_train[categorical_cols].astype('category')\n",
    "X_test[categorical_cols] = X_test[categorical_cols].astype('category')\n",
    "binary_cols = [col for col in X_train.columns if col.endswith('_bin')]\n",
    "X_train[binary_cols] = X_train[binary_cols].astype(bool)\n",
    "X_test[binary_cols] = X_test[binary_cols].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_imputer(X_train, X_val, categorical_columns=None, numerical_columns=None):\n",
    "    \"\"\"\n",
    "    Impute missing values in train and validation/test sets using only training data statistics.\n",
    "    - categorical_columns: list of categorical columns to impute with mode\n",
    "    - numerical_columns: list of numerical columns to impute with mean\n",
    "    \"\"\"\n",
    "    X_train_imputed = X_train.copy()\n",
    "    X_val_imputed = X_val.copy()\n",
    "\n",
    "    # Categorical columns\n",
    "    if categorical_columns:\n",
    "        for col in categorical_columns:\n",
    "            if col in X_train_imputed.columns:\n",
    "                mode_value = X_train_imputed[col].mode(dropna=True)\n",
    "                if not mode_value.empty:\n",
    "                    mode_value = mode_value[0]\n",
    "                    X_train_imputed[col].fillna(mode_value, inplace=True)\n",
    "                    if col in X_val_imputed.columns:\n",
    "                        X_val_imputed[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "    # Numerical columns\n",
    "    if numerical_columns:\n",
    "        for col in numerical_columns:\n",
    "            if col in X_train_imputed.columns:\n",
    "                mean_value = X_train_imputed[col].mean(skipna=True)\n",
    "                if pd.notna(mean_value):  # ensure mean is valid\n",
    "                    X_train_imputed[col].fillna(mean_value, inplace=True)\n",
    "                    if col in X_val_imputed.columns:\n",
    "                        X_val_imputed[col].fillna(mean_value, inplace=True)\n",
    "\n",
    "    return X_train_imputed, X_val_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in X_train.columns if not col.endswith(('_cat', '_bin'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed,X_test_imputed = simple_imputer(X_train, X_test, categorical_cols,num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_anova_chi = ['ps_car_13', 'ps_reg_02', 'ps_car_12', 'feature4', 'ps_reg_03', 'feature2', 'ps_car_15', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01', 'feature5', 'ps_car_14', 'feature7', 'ps_ind_03', 'ps_calc_01', 'ps_car_04_cat', 'ps_ind_05_cat', 'ps_car_11_cat', 'ps_car_06_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_ind_04_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_05_cat', 'ps_ind_17_bin', 'ps_ind_07_bin', 'ps_ind_06_bin', 'ps_ind_16_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_12_bin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_features = X_train_imputed[selected_features_anova_chi]\n",
    "X_test_selected_features = X_test_imputed[selected_features_anova_chi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_cat_features = ['ps_car_04_cat', 'ps_ind_05_cat', 'ps_car_11_cat', 'ps_car_06_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_ind_04_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_05_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:296: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 4 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 9 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 13 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "d:\\ML project\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:397: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 14 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical NB Accuracy: 0.9074417924220429\n",
      "Categorical NB F1 Score: 0.9077138411196586\n",
      "Categorical NB AUROC: 0.6206718046120854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Separate categorical and numerical columns\n",
    "categorical_cols = [col for col in selected_features_anova_chi if col.endswith('_cat')]\n",
    "binary_cols = [col for col in selected_features_anova_chi if col.endswith('_bin')]\n",
    "numerical_cols = [col for col in selected_features_anova_chi \n",
    "                  if col not in categorical_cols + binary_cols]\n",
    "\n",
    "# subset the data\n",
    "X_train_sel = X_train_imputed[selected_features_anova_chi].copy()\n",
    "X_val_sel = X_test_imputed[selected_features_anova_chi].copy()\n",
    "y_train_sel = y_train.copy()\n",
    "y_val_sel = y_test.copy()\n",
    "\n",
    "# ensure categorical features are integer-encoded\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_sel[col] = le.fit_transform(X_train_sel[col].astype(str))\n",
    "    X_val_sel[col] = le.transform(X_val_sel[col].astype(str))\n",
    "\n",
    "# CategoricalNB expects all features to be discrete\n",
    "# So we'll discretize numeric ones (optional, but helps model)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kb = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "X_train_sel[numerical_cols] = kb.fit_transform(X_train_sel[numerical_cols])\n",
    "X_val_sel[numerical_cols] = kb.transform(X_val_sel[numerical_cols])\n",
    "\n",
    "# Train Categorical Naive Bayes\n",
    "cat_nb = CategoricalNB()\n",
    "cat_nb.fit(X_train_sel, y_train_sel)\n",
    "\n",
    "# Predict\n",
    "y_pred = cat_nb.predict(X_val_sel)\n",
    "y_proba = cat_nb.predict_proba(X_val_sel)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Categorical NB Accuracy:\", accuracy_score(y_val_sel, y_pred))\n",
    "print(\"Categorical NB F1 Score:\", f1_score(y_val_sel, y_pred, average='weighted'))\n",
    "print(\"Categorical NB AUROC:\", roc_auc_score(y_val_sel, y_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13944237,
     "sourceId": 116866,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
